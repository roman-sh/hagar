# Refactor Plan: Concurrent Document Processing (Simplified)

## 1. Objective
To refactor the application to support concurrent backend processing of multiple documents per user, while ensuring the user experiences a clean, serialized conversation for one document at a time. This plan outlines a simplified architecture that achieves this without the complexity of virtual agents or a gatekeeper.

## 2. Core Architecture
The architecture separates background processing from user interaction using a combination of data isolation (`docId`) and a dedicated service to manage a user-specific, serialized conversation queue.

- **`docId` as Context Key:** Every message in the `messages` collection will be stamped with the `docId` of the document it relates to. This allows for independent, parallel processing contexts.
- **AI Context from Trigger:** The AI's working context for any given task is determined by the `docId` passed into `gpt.process` from the triggering event (inbound message). This allows the AI to work on any document at any time.
- **Serialized Conversation Queue (`PhoneContextManager`):** A new service, `PhoneContextManager`, will manage a simple FIFO queue of `docId`s for each user (`context_queue:<phone>`). This queue's **sole purpose** is to control which document's outbound messages are currently being sent to the user, ensuring a clean, non-interleaved conversational experience.
- **Pausable Outbound Queues:** Messages generated by the AI are sent to per-document, pausable queues. The `PhoneContextManager`'s active context determines which of these queues is currently unpaused and delivering messages to the user.

## 3. Implementation Plan

### Step 1: Data Model and `docId` Stamping
- **Update `MessageDocument` Type:** In `src/types/documents.ts`, add an optional `docId?: string` field.
- **Update `scan-validation.ts`:** Ensure the initial trigger message created in the processor has its `docId` correctly stamped.
- **Update `gpt.ts`:**
    - The `saveMessages` function must be updated to stamp all new messages it creates with the `contextId`.
    - A `metadata` object will be created and passed to tools.

### Step 2: Implement Hybrid Context Logic in `gpt.process`
- **Modify `gpt.process`:** This function will become the single source of truth for context determination.
    1.  Its signature will be updated to accept an optional `docId`: `gpt.process({ phone, docId? })`.
    2.  **If `docId` is provided** (by a system trigger like `scan-validation`), it will be used directly to filter the message history.
    3.  **If `docId` is `undefined`** (from a user message via the debouncer), the function will call `phoneContextManager.getCurrent(phone)` to retrieve the active `docId`. This may return `null` if no document is active.
    4.  All messages generated during this turn will be stamped with the determined context (`docId` or `null`).

### Step 3: Implement the `PhoneContextManager` Service
- **Create New Service:** Create a new service at `src/services/phone-context-manager.ts` to manage the user-facing context queue. This service will be backed by a Redis List (`context_queue:<phone>`).
- **Service Methods:** The service should expose methods like:
    - `enqueue(phone, docId)`
    - `getCurrent(phone)`
    - `rotate(phone)`: Removes the old `docId` from the head of the queue, then peeks at the new head and resumes its corresponding outbound queue. This makes it the central orchestrator of a context switch.
- **`document.onboard`:** Update this function to call `phoneContextManager.enqueue(phone, docId)`.

### Step 4: Implement "Direct Buffer" Architecture
- **`messageBufferManager` Service:** The `messageBufferManager` will be created to manage a pool of dynamic, per-document **Bull queues**. It will expose methods like `getQueue`, `enqueue`, `pause`, `resume`, and `destroy`.
- **No Central Queue:** The concept of a central `outboundMessagesQueue` is **eliminated**. The per-document Bull queues will be responsible for sending messages directly to the user.
- **Rate Limiting:** Each per-document Bull queue will be created with `concurrency: 1`, ensuring messages for any single document are sent in order. This is a pragmatic solution for the current scale, with the understanding that a future WhatsApp Business API migration would necessitate a different global rate-limiting strategy.
- **`document.onboard`:** Update this function to call `phoneContextManager.enqueue(phone, docId)`.
- **Event-Driven, AI-Initiated Rotation:**
    - The rotation process will be event-driven, initiated by the AI but executed by the system in a reliable sequence.
    - **Two-Turn AI Flow:**
        1.  **Turn 1:** The AI calls a new `scheduleRotation({ summary })` tool. This tool saves the summary and **dynamically registers a temporary, one-time listener** on the correct per-document buffer queue. It then returns a success message.
        2.  **Turn 2:** In response to the tool's success, the AI generates its final, user-facing conversational message.
- **Update `gpt.ts` Sending Logic:** `gpt.ts` will send all messages via `messageBufferManager.enqueue(contextId, message)`.
- **Context Switch Orchestration (Dynamic Listener):**
    - The `scheduleRotation` tool will get the specific Bull queue for the current `contextId` from the `messageBufferManager`.
    - It will use Bull's `.once('active', listener)` method to register a temporary listener **directly on that per-document queue**.
    - This listener will fire when the final message for that document begins processing. Its only job is to call `phoneContextManager.rotate(phone)`.
    - After executing once, the listener is automatically de-registered by Bull.
    - The `phoneContextManager.rotate(phone)` method remains the orchestrator, calling `messageBufferManager.pause(oldDocId)` and `messageBufferManager.resume(newDocId)`.
